{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Dependencies\n",
    "\n",
    "First, we'll set up our environment by installing the necessary Python libraries.\n",
    "\n",
    "* **`unsloth`**: We install the latest version of Unsloth directly from its GitHub repository. Unsloth provides massive speedups and memory reductions for fine-tuning LLMs, enabling us to train models up to 2x faster and use 60% less memory. The `[colab-new]` option ensures compatibility with the latest Google Colab environments.\n",
    "* **Hugging Face Ecosystem**: We install key libraries for training and optimization:\n",
    "    * `peft`: Parameter-Efficient Fine-Tuning, for using techniques like LoRA.\n",
    "    * `trl`: Transformer Reinforcement Learning, for its easy-to-use `SFTTrainer`.\n",
    "    * `accelerate`: To easily run our training script on any hardware.\n",
    "    * `bitsandbytes`: For 4-bit quantization (QLoRA), which drastically reduces model size.\n",
    "* **`xformers`**: Provides memory-efficient attention mechanisms for another performance boost.\n",
    "* **`wandb`**: Weights & Biases, for logging our experiments and tracking metrics like training loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\" -q\n",
    "!pip install --no-deps xformers trl peft accelerate bitsandbytes -q\n",
    "!pip install wandb -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log In to Services\n",
    "\n",
    "To download our model and log our training progress, we need to authenticate with Hugging Face and Weights & Biases (W&B). We use Colab's `userdata` to securely access our API keys without hardcoding them in the notebook.\n",
    "\n",
    "* **Hugging Face Hub**: We need to log in to download the Phi-3 model, which requires accepting user conditions.\n",
    "* **Weights & Biases**: We log in to `wandb` to enable experiment tracking. This will allow us to monitor metrics like training loss in real-time.\n",
    "\n",
    "> **Action Required:** Before running this cell, you must store your API keys as secrets in Google Colab.\n",
    "> 1.  Click the **🔑 (Secrets)** icon on the left sidebar.\n",
    "> 2.  Create a new secret named `hugging` and paste your Hugging Face access token (with `write` permissions) as the value.\n",
    "> 3.  Create another secret named `WANDB_API_KEY` and paste your W&B API key as the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /teamspace/studios/this_studio/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjavaemailacount\u001b[0m (\u001b[33mjavaemailacount-none\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Log in to huggingface\n",
    "from google.colab import userdata\n",
    "hf_token = userdata.get('hugging')\n",
    "\n",
    "# Log in to wandb\n",
    "import wandb\n",
    "wandb_api_key = userdata.get('WANDB_API_KEY')\n",
    "wandb.login(key=wandb_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Core Libraries\n",
    "\n",
    "With our environment set up and authenticated, we can now import the core components from the libraries we installed. Each of these plays a critical role in the fine-tuning pipeline.\n",
    "\n",
    "* **`FastLanguageModel`**: The star of the show from Unsloth. This class will load our base model and automatically apply all the necessary optimizations for fast, memory-efficient training.\n",
    "* **`torch`**: The fundamental deep learning framework.\n",
    "* **`load_dataset`**: A function from the Hugging Face `datasets` library to easily pull our training data from the Hub.\n",
    "* **`SFTTrainer`**: A specialized trainer from the `trl` library designed specifically for Supervised Fine-Tuning.\n",
    "* **`TrainingArguments`**: A configuration class from `transformers` where we will define all the hyperparameters for our training job.\n",
    "* **`is_bfloat16_supported`**: A utility from Unsloth to check if our hardware supports `bfloat16` precision, which is ideal for training modern transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:xformers:WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.8.0+cu128 with CUDA 1208 (you have 2.7.1+cu128)\n",
      "    Python  3.9.23 (you have 3.10.10)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model and Tokenizer\n",
    "\n",
    "Now we use Unsloth's `FastLanguageModel` to load our pre-trained model. This single, powerful command handles several critical steps for us:\n",
    "\n",
    "1.  **Downloads the model** from the Hugging Face Hub.\n",
    "2.  **Applies 4-bit quantization** to drastically reduce memory usage.\n",
    "3.  **Patches the model** with performance optimizations for faster training.\n",
    "4.  **Prepares the tokenizer** for use in training.\n",
    "\n",
    "Let's look at the key parameters:\n",
    "* `model_name`: We are loading `\"microsoft/Phi-3-mini-4k-instruct\"`, a highly capable small language model that is perfect for fine-tuning on consumer hardware.\n",
    "* `load_in_4bit = True`: This is the core of our memory-saving strategy. It enables 4-bit quantization (QLoRA), reducing the VRAM footprint significantly.\n",
    "* `max_seq_length = 2048`: We set the maximum context window for our training examples. This offers a good balance between capturing long-range dependencies and managing memory.\n",
    "* `dtype = None`: This allows Unsloth to automatically detect and use the optimal data type for our GPU (like `bfloat16`), ensuring the best possible training performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.8.9: Fast Mistral patching. Transformers: 4.55.4.\n",
      "   \\\\   /|    NVIDIA L40S. Num GPUs = 1. Max memory: 44.527 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "# Load model (same as before)\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    max_seq_length = 2048,\n",
    "    load_in_4bit=True,\n",
    "    dtype=None,\n",
    "    token = hf_token,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Base Model (Before Fine-Tuning)\n",
    "\n",
    "Before we fine-tune the model, it's crucial to establish a baseline. We need to see how the pre-trained model performs on a task similar to our goal. This helps us understand its out-of-the-box capabilities and gives us a \"before\" snapshot to compare against our \"after\" fine-tuned version.\n",
    "\n",
    "Our test process involves a few key steps:\n",
    "1.  **Craft a Prompt**: We create a sample conversation using the standard `system` and `user` roles. The system prompt sets the model's persona (an expert financial analyst), while the user prompt provides a specific context and a question.\n",
    "2.  **Apply Chat Template**: We use `tokenizer.apply_chat_template`. This is a critical function that formats our structured conversation into the exact string format that `Phi-3-instruct` expects, including special tokens.\n",
    "3.  **Generate Response**: We run a standard inference using `model.generate()` to get the model's answer based on our prompt.\n",
    "4.  **Evaluate Output**: We'll examine the response to see if the model correctly follows instructions and extracts the required information from the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test base model first to ensure it works\n",
    "def test_base_model():\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\", \n",
    "            \"content\": \"You are an expert financial analyst. Answer the user's question based only on the provided context.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": \"\"\"Context: The company's gross margin improved to 45.8% in fiscal year 2023, up from 42.1% in the prior year. The margin expansion was mainly attributable to a favorable product mix with higher sales of our premium software subscriptions, and manufacturing efficiencies gained from our new automated production line in Alexandria, Egypt.\n",
    "\n",
    "Question: What were the two key factors that contributed to the increase in the company's gross margin in fiscal year 2023?\"\"\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Use the model's built-in chat template\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    print(\"Generated prompt:\")\n",
    "    print(prompt)\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=128,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode only new tokens\n",
    "    response_tokens = outputs[0][inputs[\"input_ids\"].shape[-1]:]\n",
    "    response = tokenizer.decode(response_tokens, skip_special_tokens=True)\n",
    "    print(\"Base model response:\", response)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing base model...\n",
      "Generated prompt:\n",
      "<|system|>\n",
      "You are an expert financial analyst. Answer the user's question based only on the provided context.<|end|>\n",
      "<|user|>\n",
      "Context: The company's gross margin improved to 45.8% in fiscal year 2023, up from 42.1% in the prior year. The margin expansion was mainly attributable to a favorable product mix with higher sales of our premium software subscriptions, and manufacturing efficiencies gained from our new automated production line in Alexandria, Egypt.\n",
      "\n",
      "Question: What were the two key factors that contributed to the increase in the company's gross margin in fiscal year 2023?<|end|>\n",
      "<|assistant|>\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model response: The two key factors that contributed to the increase in the company's gross margin in fiscal year 2023 were a favorable product mix, which included higher sales of premium software subscriptions, and manufacturing efficiencies gained from the introduction of a new automated production line in Alexandria, Egypt.\n"
     ]
    }
   ],
   "source": [
    "# Test base model first\n",
    "print(\"Testing base model...\")\n",
    "base_response = test_base_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure LoRA for Efficient Fine-Tuning\n",
    "\n",
    "Now we get to the core of Parameter-Efficient Fine-Tuning (PEFT). Instead of training the entire model, we'll use **Low-Rank Adaptation (LoRA)** to inject small, trainable \"adapter\" matrices into the model's architecture. This means we only need to train a tiny fraction of the total parameters (typically <1%), which is what makes fine-tuning feasible on a single GPU.\n",
    "\n",
    "Unsloth's `get_peft_model` function seamlessly applies this configuration to our 4-bit model. Let's look at the key hyperparameters:\n",
    "\n",
    "* `r = 16`: The rank or dimension of the LoRA adapter matrices. A higher rank means more trainable parameters and greater expressive power, but also more memory. `16` is a solid and popular choice.\n",
    "* `lora_alpha = 16`: The scaling factor for the LoRA weights. A common convention is to set this equal to `r`.\n",
    "* `target_modules`: This is a critical setting. We specify the names of the layers (in this case, the attention and feed-forward layers) where the LoRA adapters will be injected. Unsloth provides a utility to find all potential layers, and we're targeting the most impactful ones here.\n",
    "* `use_gradient_checkpointing = \"unsloth\"`: A crucial memory-saving technique that trades a bit of computation time to drastically reduce VRAM usage, allowing us to use larger batch sizes or longer sequences. The `\"unsloth\"` option enables a custom, faster implementation.\n",
    "* `random_state = 2002`: We set a seed for reproducibility. Fun fact: 2002 was the year the modern Bibliotheca Alexandrina was inaugurated, not too far from our model's fictional manufacturing plant in Alexandria.\n",
    "\n",
    "After this cell, our model is fully prepared for training. The original weights are frozen, and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.8.9 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "# Configure LoRA (same as before)\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model = model,\n",
    "    r = 16,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 2002,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Prepare the Dataset\n",
    "\n",
    "The quality and format of your training data are paramount for a successful fine-tune. Instruction-tuned models like Phi-3 are highly sensitive to the prompt format they were trained on. In this step, we will load our financial Q&A dataset and transform each entry to perfectly match Phi-3's specific chat template.\n",
    "\n",
    "Our workflow is as follows:\n",
    "1.  **Load Dataset**: We start by loading the `virattt/llama-3-8b-financialQA` dataset from the Hugging Face Hub. This dataset contains pairs of financial contexts, questions, and expert answers.\n",
    "2.  **Define a Formatting Function**: We create a function, `formatting_prompts_func`, that takes a batch of examples and restructures them. For each row, it builds a conversation with three parts:\n",
    "    * A `system` message to consistently set the model's persona.\n",
    "    * A `user` message combining the `context` and `question`.\n",
    "    * An `assistant` message containing the ground-truth `answer` that we want the model to learn.\n",
    "3.  **Apply the Chat Template**: Inside the function, we use the crucial `tokenizer.apply_chat_template` method. This converts the structured conversation into"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXED: Use Phi-3's actual chat template for training\n",
    "def formatting_prompts_func(examples):\n",
    "    questions = examples[\"question\"]\n",
    "    contexts = examples[\"context\"]  \n",
    "    responses = examples[\"answer\"]\n",
    "    texts = []\n",
    "    \n",
    "    for question, context, response in zip(questions, contexts, responses):\n",
    "        # Create proper conversation format\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are an expert financial analyst. Answer the user's question based only on the provided context.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": f\"Context: {context}\\n\\nQuestion: {question}\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": response\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Use the model's chat template\n",
    "        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "        texts.append(text)\n",
    "    \n",
    "    return {\"text\": texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample before formatting: {'question': 'What area did NVIDIA initially focus on before expanding to other computationally intensive fields?', 'answer': 'NVIDIA initially focused on PC graphics.', 'context': 'Since our original focus on PC graphics, we have expanded to several other large and important computationally intensive fields.', 'ticker': 'NVDA', 'filing': '2023_10K'}\n"
     ]
    }
   ],
   "source": [
    "# Load and format dataset\n",
    "dataset = load_dataset(\"virattt/llama-3-8b-financialQA\", split=\"train\")\n",
    "print(\"Sample before formatting:\", dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample after formatting: <|system|>\n",
      "You are an expert financial analyst. Answer the user's question based only on the provided context.<|end|>\n",
      "<|user|>\n",
      "Context: Since our original focus on PC graphics, we have expanded to several other large and important computationally intensive fields.\n",
      "\n",
      "Question: What area did NVIDIA initially focus on before expanding to other computationally intensive fields?<|end|>\n",
      "<|assistant|>\n",
      "NVIDIA initially focused on PC graphics.<|end|>\n",
      "<|endoftext|>...\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
    "print(\"Sample after formatting:\", dataset[0][\"text\"][:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure and Launch the Fine-Tuning Job\n",
    "\n",
    "We have arrived at the final step. With our model loaded, LoRA configured, and the dataset perfectly formatted, we can now set up the trainer and launch the fine-tuning process.\n",
    "\n",
    "We will use the `SFTTrainer` from the TRL library, which handles the complexities of the training loop for us. The behavior of the trainer is controlled by a comprehensive set of `TrainingArguments`.\n",
    "\n",
    "#### Key Hyperparameters:\n",
    "* **Batching**: We use a `per_device_train_batch_size` of 2 and `gradient_accumulation_steps` of 4. This gives us an effective batch size of `2 * 4 = 8`, which helps stabilize training while keeping memory usage low.\n",
    "* **Training Steps**: We set `max_steps = 60` for a short, demonstrative training run. In a real-world scenario, you would train for more steps or for a certain number of epochs.\n",
    "* **Learning Rate**: A `learning_rate` of `2e-4` with a linear scheduler and a few `warmup_steps` is a standard and effective setup for LoRA.\n",
    "* **Optimizations**: We use the `adamw_8bit` optimizer and enable `bf16` (bfloat16 mixed-precision) if our GPU supports it. These are powerful techniques that accelerate training and reduce memory consumption.\n",
    "* **Logging and Saving**: We `logging_steps = 1` to see the loss at every step and will save a model checkpoint to the `outputs` directory halfway through training (`save_steps = 30`).\n",
    "\n",
    "#### Launching the Training\n",
    "With all the components in place, a single call to `trainer.train()` starts the fine-tuning process. As the training commences, keep an eye on the logged training loss—it should steadily decrease, indicating that the model is learning from our financial Q&A data. Let's kick it off and let the GPU work its magic through the early hours of this Saturday morning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = 2048,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False,\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        max_steps = 60,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 2002,\n",
    "        output_dir = \"outputs\",\n",
    "        save_strategy = \"steps\",\n",
    "        save_steps = 30,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 7,000 | Num Epochs = 1 | Total steps = 60\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 29,884,416 of 3,850,963,968 (0.78% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/teamspace/studios/this_studio/wandb/run-20250822_210412-24g6el6l</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/javaemailacount-none/huggingface/runs/24g6el6l' target=\"_blank\">apricot-blaze-10</a></strong> to <a href='https://wandb.ai/javaemailacount-none/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/javaemailacount-none/huggingface' target=\"_blank\">https://wandb.ai/javaemailacount-none/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/javaemailacount-none/huggingface/runs/24g6el6l' target=\"_blank\">https://wandb.ai/javaemailacount-none/huggingface/runs/24g6el6l</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 01:05, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.906300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.575400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>5.254500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.470500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>5.033000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.432900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.265200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.738600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.693600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.447100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.588700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.203000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.154500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.576000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.108200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.863000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.499400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.745900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.223200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.956100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.178800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.074700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.038400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.027800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.153200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.021500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.023400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.021300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.004900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.246000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.011800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.066800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.196100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.005500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.010100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.002600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.016100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.063800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>1.092700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.017400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.001900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.765900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.003400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.051400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.002700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.014900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.003700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.001900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.003000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.001300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.002300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.001300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.001300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.002500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.011100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.003600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.003500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.001700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.001400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "print(\"Starting training...\")\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Fine-Tuned Model\n",
    "\n",
    "The training is complete! Now for the moment of truth: did our fine-tuning work? We will now test our specialized model and compare its performance directly against the baseline we established in Step 5.\n",
    "\n",
    "To ensure a fair evaluation, our process is simple but critical:\n",
    "1.  **Use a Consistent Prompt Format**: Our new `inference` function formats the prompt using the **exact same** system message and chat template that the model was trained on. This consistency is crucial for unlocking the model's new capabilities.\n",
    "2.  **Rerun the Original Test Case**: We will ask the **exact same question** using the same context from our baseline test.\n",
    "\n",
    "This provides our \"after\" snapshot. Compare this response to the one from the base model. Look for improvements in accuracy, conciseness, formatting (e.g., using a proper list), and overall adherence to the system prompt's instructions.\n",
    "\n",
    "After a short but intense training session in the quiet of the Giza night, let's see how our newly specialized financial analyst performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXED: Proper inference function\n",
    "def inference(question, context):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are an expert financial analyst. Answer the user's question based only on the provided context.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Context: {context}\\n\\nQuestion: {question}\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Use the same chat template as training\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=128,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            repetition_penalty=1.1\n",
    "        )\n",
    "    \n",
    "    # Extract only the generated response\n",
    "    response_tokens = outputs[0][inputs[\"input_ids\"].shape[-1]:]\n",
    "    response = tokenizer.decode(response_tokens, skip_special_tokens=True)\n",
    "    \n",
    "    return response.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Testing fine-tuned model...\n",
      "Question: What were the two key factors that contributed to the increase in the company's gross margin in fiscal year 2023?\n",
      "Response: The two key factors that contributed to the increase in the company's gross margin in fiscal year 2023 were (a) a more favorable product mix with higher sales of their premium software subscriptions, which indicates better pricing power or market preference for these high-margin products; and (b) manufacturing efficiencies gained from implementing a new automated production line in Alexandria, Egypt, suggesting cost savings through increased operational efficiency.\n"
     ]
    }
   ],
   "source": [
    "# Test the fine-tuned model\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Testing fine-tuned model...\")\n",
    "\n",
    "context = \"The company's gross margin improved to 45.8% in fiscal year 2023, up from 42.1% in the prior year. The margin expansion was mainly attributable to a favorable product mix with higher sales of our premium software subscriptions, and manufacturing efficiencies gained from our new automated production line in Alexandria, Egypt.\"\n",
    "question = \"What were the two key factors that contributed to the increase in the company's gross margin in fiscal year 2023?\"\n",
    "\n",
    "response = inference(question, context)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔬 <span style=\"color: #007bff;\">Analysis</span>: The <span style=\"color: #28a745;\">Impact</span> of Fine-Tuning\n",
    "\n",
    "The results above clearly demonstrate the value of Supervised Fine-Tuning (SFT). While the base `Phi-3` model was able to extract the correct facts, our fine-tuned model learned to adopt the **persona of an expert financial analyst**.\n",
    "\n",
    "Let's compare the outputs.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 Base Model (Before Fine-Tuning)\n",
    "<div style=\"background-color: #262730; color: #EAEAEA; border: 1px solid #444; border-radius: 8px; padding: 15px; margin: 10px 0;\">\n",
    "The two key factors that contributed to the increase in the company's gross margin in fiscal year 2023 were a favorable product mix, which included higher sales of premium software subscriptions, and manufacturing efficiencies gained from the introduction of a new automated production line in Alexandria, <span style=\"color: #ffc107;\">Egypt</span>.\n",
    "</div>\n",
    "\n",
    "### 🏆 Fine-Tuned Model (After Fine-Tuning)\n",
    "<div style=\"background-color: #262730; color: #EAEAEA; border: 1px solid #444; border-radius: 8px; padding: 15px; margin: 10px 0;\">\n",
    "The two key factors that contributed to the increase in the company's gross margin in fiscal year 2023 were (a) a more favorable product mix with higher sales of their premium software subscriptions, <span style=\"background-color: #1a472a; color: #98FB98; padding: 2px 4px; border-radius: 4px;\">**which indicates better pricing power or market preference for these high-margin products;**</span> and (b) manufacturing efficiencies gained from implementing a new automated production line in Alexandria, <span style=\"color: #ffc107;\">Egypt</span>, <span style=\"background-color: #1a472a; color: #98FB98; padding: 2px 4px; border-radius: 4px;\">**suggesting cost savings through increased operational efficiency.**</span>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "### ✨ <span style=\"color: #007bff;\">Key Improvements</span> from Fine-Tuning:\n",
    "\n",
    "<div style=\"border-left: 3px solid #007bff; padding-left: 15px; margin-top: 10px;\">\n",
    "\n",
    "* **<span style=\"color: #28a745;\">Persona Adoption & Analytical Tone</span>**: The base model simply stated the facts. The fine-tuned model speaks like an analyst, using phrases like <code style=\"background-color: #282c34; color: #e5c07b; padding: 2px 5px; border-radius: 4px;\">indicates better pricing power</code> and <code style=\"background-color: #282c34; color: #e5c07b; padding: 2px 5px; border-radius: 4px;\">suggesting cost savings</code>.\n",
    "\n",
    "* **<span style=\"color: #dc3545;\">Interpretive Depth</span>**: Instead of just extracting information, the fine-tuned model now **interprets** it. It explains the *implications* of the facts (e.g., higher sales of premium products lead to higher margins), which is a much higher-level skill.\n",
    "\n",
    "* **<span style=\"color: #ffc107;\">Improved Structure</span>**: The fine-tuned model learned to structure its answer more clearly using `(a)` and `(b)`, making the two key points distinct and easier to read for a professional audience.\n",
    "\n",
    "</div>\n",
    "\n",
    "By showing the model hundreds of examples from our financial Q&A dataset, we didn't just teach it new facts; we taught it **how to think, speak, and structure information** like an expert financial analyst. This is the true power of fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge, Save, and Package the Final Model\n",
    "\n",
    "Our work is not complete until the model is saved and ready for deployment. The fine-tuning process created lightweight LoRA \"adapter\" weights, which are separate from the original base model. For easy, portable inference, we need to merge these adapters back into the base model to create a single, unified set of weights.\n",
    "\n",
    "This final step effectively \"bakes in\" our specialized financial knowledge.\n",
    "\n",
    "1.  **Merge and Unload**: We call `model.merge_and_unload()`. This powerful Unsloth function performs two actions:\n",
    "    * **Merges** the trained LoRA weights directly into the base model's attention and MLP layers.\n",
    "    * **Unloads** the PEFT wrapper, returning a standard Hugging Face `PreTrainedModel` object. This new object is a complete, standalone model that doesn't require the `peft` library for inference.\n",
    "2.  **Save the Model**: We use the standard `save_pretrained` method to save our new, merged model. We specify `safe_serialization=True` to use the modern and secure `safetensors` format.\n",
    "3.  **Save the Tokenizer**: Crucially, we also save the tokenizer in the same directory. The model weights and the tokenizer are a pair; you need both to correctly run inference.\n",
    "\n",
    "With the first light of dawn approaching over the Giza plateau, our final, expert financial analyst model is now serialized to disk, ready to be uploaded, shared, and deployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create directory for the complete fine-tuned model\n",
    "save_directory = \"Data/complete_finetuned_model\"\n",
    "os.makedirs(save_directory, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging LoRA adapter with base model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/peft/tuners/lora/bnb.py:348: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print(\"Merging LoRA adapter with base model...\")\n",
    "# Merge the LoRA adapter with the base model\n",
    "# This combines the original weights with the learned LoRA weights\n",
    "merged_model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving merged model and tokenizer...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Data/complete_finetuned_model/tokenizer_config.json',\n",
       " 'Data/complete_finetuned_model/special_tokens_map.json',\n",
       " 'Data/complete_finetuned_model/chat_template.jinja',\n",
       " 'Data/complete_finetuned_model/tokenizer.model',\n",
       " 'Data/complete_finetuned_model/added_tokens.json',\n",
       " 'Data/complete_finetuned_model/tokenizer.json')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Saving merged model and tokenizer...\")\n",
    "\n",
    "# Save the complete merged model\n",
    "merged_model.save_pretrained(\n",
    "    save_directory,\n",
    "    safe_serialization=True,  # Use safetensors format (recommended)\n",
    "    max_shard_size=\"2GB\"      # Split large models into 2GB chunks\n",
    ")\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(save_directory)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
