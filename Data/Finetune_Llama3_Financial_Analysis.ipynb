{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55d562a8-8635-4593-a27e-ec398844697c",
   "metadata": {},
   "source": [
    "# Environment Setup and Dependencies Installation\n",
    "\n",
    "## Installing Core Libraries\n",
    "\n",
    "This section installs the essential libraries required for fine-tuning language models with Unsloth, including optimization tools and monitoring capabilities.\n",
    "\n",
    "**Key Components:**\n",
    "- **Unsloth**: Efficient fine-tuning framework for large language models\n",
    "- **XFormers**: Memory-efficient attention implementations\n",
    "- **TRL**: Transformer Reinforcement Learning library\n",
    "- **PEFT**: Parameter-Efficient Fine-Tuning methods\n",
    "- **Accelerate**: Multi-GPU and distributed training support\n",
    "- **BitsAndBytes**: Quantization library for memory optimization\n",
    "- **WandB**: Experiment tracking and monitoring platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9d70ff5-e254-4bb3-8ae2-3d0897f0832d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\" -q\n",
    "!pip install --no-deps xformers trl peft accelerate bitsandbytes -q\n",
    "!pip install wandb -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fecee48-03e5-4396-9222-5c5cf3e3f1c4",
   "metadata": {},
   "source": [
    "# Authentication and API Configuration\n",
    "\n",
    "## Service Authentication Setup\n",
    "\n",
    "This section handles secure authentication for external services required during the fine-tuning process.\n",
    "\n",
    "**Authentication Components:**\n",
    "- **Hugging Face Hub**: Retrieves API token from Colab secrets for model repository access and upload permissions\n",
    "- **Weights & Biases (WandB)**: Authenticates with experiment tracking platform for monitoring training metrics, logging experiments, and visualizing results\n",
    "\n",
    "**Security Implementation:** API keys are securely stored in Google Colab's userdata secrets management system, ensuring credentials are not exposed in the notebook code or version control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ced89bc-39c1-49bd-9eff-ff7c27180a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /teamspace/studios/this_studio/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjavaemailacount\u001b[0m (\u001b[33mjavaemailacount-none\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Log in to huggingface\n",
    "from google.colab import userdata\n",
    "hf_api = userdata.get('hugging')\n",
    "\n",
    "# Log in to wandb\n",
    "import wandb\n",
    "wandb_api_key = userdata.get('WANDB_API_KEY')\n",
    "wandb.login(key=wandb_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cffecf-532b-4ca2-bb76-bdd7d676dca5",
   "metadata": {},
   "source": [
    "# Fine-Tuning Framework Imports\n",
    "\n",
    "## Core Fine-Tuning Libraries\n",
    "\n",
    "This section imports the essential components required for supervised fine-tuning of large language models using the Unsloth framework.\n",
    "\n",
    "**Primary Components:**\n",
    "- **FastLanguageModel**: Unsloth's optimized model wrapper for efficient fine-tuning\n",
    "- **PyTorch**: Core deep learning framework for tensor operations and GPU acceleration\n",
    "- **Datasets**: Hugging Face library for loading and processing training datasets\n",
    "- **SFTTrainer**: Supervised Fine-Tuning trainer from the TRL library\n",
    "- **TrainingArguments**: Configuration class for defining training hyperparameters\n",
    "- **Hardware Detection**: Utility function to check BFloat16 support for optimal performance\n",
    "\n",
    "**Performance Note:** These imports enable memory-efficient fine-tuning with automatic mixed precision and hardware optimization detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9b0d2fb-df40-42d6-b5d8-a4a22757ef8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:xformers:WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.8.0+cu128 with CUDA 1208 (you have 2.7.1+cu128)\n",
      "    Python  3.9.23 (you have 3.10.10)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462bf456-6953-4507-ba52-488ebfb50b08",
   "metadata": {},
   "source": [
    "# Pre-Trained Model and Tokenizer Initialization\n",
    "\n",
    "## Loading Meta-Llama-3-8B-Instruct Model\n",
    "\n",
    "This section initializes the pre-trained language model and its corresponding tokenizer from Hugging Face Hub using optimized loading configurations.\n",
    "\n",
    "**Model Specifications:**\n",
    "- **Base Model**: [Meta-Llama-3-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct)\n",
    "- **Architecture**: Llama 3 with 8 billion parameters, instruction-tuned variant\n",
    "- **Context Length**: 2048 tokens maximum sequence length\n",
    "- **Quantization**: 4-bit precision for memory efficiency (reduces VRAM usage by ~75%)\n",
    "- **Data Type**: Auto-detection based on hardware capabilities (optimizes for available GPU)\n",
    "\n",
    "**Performance Optimization:**\n",
    "- 4-bit quantization significantly reduces memory footprint while maintaining model quality\n",
    "- Automatic dtype selection ensures optimal performance across different hardware configurations\n",
    "- Secure authentication via Hugging Face API token for gated model access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05276dc0-9c01-4467-bd71-260a1303a948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.8.9: Fast Llama patching. Transformers: 4.55.3.\n",
      "   \\\\   /|    NVIDIA L40S. Num GPUs = 1. Max memory: 44.527 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "140ff3a9753843a6a014f2dec78379ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/5.70G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b285215e0ef340638cb16159e6b906bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/220 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1fbb1c6fae2404dbb704bca6d72db45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "101979ec980745ffbaec04a9e53ab3e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d27b458ddbbd45d5b2bc392ac9f91dd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/345 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    # Max number of tokens the model can generate in one iteration\n",
    "    max_seq_length = 2048,\n",
    "    # Enable 4-bit quantization\n",
    "    load_in_4bit=True,\n",
    "    # Auto detect data type based on the hardware\n",
    "    dtype=None,\n",
    "    # Put my huggingface api\n",
    "    token = hf_api,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1d34af-090d-4b13-80b6-0a7c2e5ad322",
   "metadata": {},
   "source": [
    "### Zero-Shot Inference Test\n",
    "#### Evaluating Pre-Trained Model Performance\n",
    "This section evaluates the performance of the base `Meta-Llama-3-8B-Instruct` model on a sample task before any fine-tuning. This process, known as zero-shot inference, establishes a crucial baseline to measure the impact and improvements gained from the subsequent fine-tuning process. The test involves providing the model with a system prompt to set its persona, a context (in this case, a financial statement excerpt), and a specific question to answer based on that context.\n",
    "\n",
    "**Inference Process Breakdown:**\n",
    "* **Prompt Structuring**: A conversational prompt is constructed using a list of messages, defining `system` and `user` roles to guide the model's response.\n",
    "* **Template Application**: The `apply_chat_template` method formats the structured prompt into the specific format required by the Llama-3 model architecture.\n",
    "* **Tokenization & Tensor Conversion**: The formatted prompt is tokenized and converted into PyTorch tensors, which are then moved to the GPU for processing.\n",
    "* **Generation Parameters**: The `model.generate` function is called with specific sampling parameters to control the output's creativity and coherence:\n",
    "    * `temperature = 0.6`: Lower values make the model more deterministic and focused.\n",
    "    * `top_p = 0.9`: Nucleus sampling, which considers the most probable tokens with a cumulative probability of 90%.\n",
    "* **Decoding**: The generated tokens are decoded back into a human-readable string, excluding special tokens, to display the final answer.\n",
    "\n",
    "**Baseline Assessment:** The output from this cell serves as the benchmark. By comparing this result with the model's output after fine-tuning on a specialized dataset, we can concretely measure the effectiveness of the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c00a2e01-6207-4232-bd28-1ee378a6775b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, the two key factors that contributed to the increase in the company's gross margin in fiscal year 2023 were:\n",
      "\n",
      "1. A favorable product mix with higher sales of premium software subscriptions.\n",
      "2. Manufacturing efficiencies gained from the new automated production line in Alexandria, Egypt.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are an expert financial analyst. Answer the user's question based only on the provided context.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"\"\"\n",
    "Context: The company's gross margin improved to 45.8% in fiscal year 2023, up from 42.1% in the prior year. The margin expansion was mainly attributable to a favorable product mix with higher sales of our premium software subscriptions, and manufacturing efficiencies gained from our new automated production line in Alexandria, Egypt.\n",
    "\n",
    "Question: What were the two key factors that contributed to the increase in the company's gross margin in fiscal year 2023?\n",
    "\"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# Format the prompt using the model's chat template\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "# Tokenize the prompt and move tensors to the GPU\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\",).to(\"cuda\")\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "# Generate a response from the base model\n",
    "# Using different sampling parameters can produce more creative results\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens = 128,\n",
    "    use_cache = True,\n",
    "    do_sample = True,\n",
    "    temperature = 0.6,\n",
    "    top_p = 0.9,\n",
    ")\n",
    "\n",
    "# Decode only the newly generated tokens\n",
    "response_tokens = outputs[0][input_ids.shape[-1]:]\n",
    "response = tokenizer.decode(response_tokens, skip_special_tokens=True)\n",
    "\n",
    "# Print the result\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b899884d-23ad-42e9-9bcd-005ec0e2e754",
   "metadata": {},
   "source": [
    "### Applying LoRA for Parameter-Efficient Fine-Tuning\n",
    "#### Configuring Low-Rank Adaptation (LoRA) Adapters\n",
    "This crucial step prepares the model for efficient fine-tuning by injecting trainable Low-Rank Adaptation (LoRA) adapters. Instead of training all 8 billion parameters of the model, this technique freezes the base model and only trains these small adapter layers. This dramatically reduces memory (VRAM) requirements and computational load, making it possible to fine-tune large models on consumer-grade hardware.\n",
    "\n",
    "**LoRA Configuration Parameters:**\n",
    "* **`r` (Rank)**: Set to `16`, this defines the rank (and thus the size) of the trainable adaptation matrices. It controls the trade-off between model adaptability and the number of new parameters.\n",
    "* **`target_modules`**: Specifies the layers of the transformer to be adapted. The selected modules (`q_proj`, `k_proj`, `v_proj`, etc.) are key components of the attention and feed-forward mechanisms, making them effective targets for fine-tuning.\n",
    "* **`lora_alpha`**: The scaling factor for the LoRA weights, also set to `16`. It modulates the magnitude of the adaptation.\n",
    "* **`lora_dropout`**: A regularization technique to prevent overfitting in the adapter layers; set to `0` to disable it.\n",
    "* **`bias`**: Set to `\"none\"`, indicating that bias parameters will not be trained, which is a common practice for LoRA.\n",
    "* **`use_gradient_checkpointing`**: A memory-saving technique that re-computes intermediate activations during the backward pass instead of storing them, enabled here with Unsloth's optimized version.\n",
    "* **`random_state`**: A seed (`2002`, the inauguration year of the Bibliotheca Alexandrina) is set to ensure the reproducibility of our fine-tuning results.\n",
    "\n",
    "**Parameter Efficiency:** By applying LoRA, we are now set to train less than 0.5% of the model's total parameters, achieving significant efficiency gains while preserving the powerful base knowledge of the original model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "145975d9-40a6-42e7-9723-b42417c9cda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.8.9 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model = model,\n",
    "    # Rank of the adaptation matrix.\n",
    "    r = 16,\n",
    "    # Specify the model layers to which LoRA adapters should be applied\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    # Scaling factor for LoRA. Controls the weight of the adaptation.\n",
    "    lora_alpha = 16,\n",
    "    # Dropout rate for LoRA.\n",
    "    lora_dropout = 0,\n",
    "    # Bias handling in LoRA. Setting to \"none\" is optimized for performance.\n",
    "    bias = \"none\",\n",
    "    # Enables gradient checkpointing to save memory during training.\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    # Seed for random number generation to ensure reproducibility of results\n",
    "    random_state = 2002,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b138adf-1ad4-4e3c-a4bb-51f79daaac0f",
   "metadata": {},
   "source": [
    "### Dataset Preparation and Prompt Formatting\n",
    "#### Creating a Custom Instruction Template\n",
    "This section focuses on preparing the training data by defining a precise prompt structure. A custom template is created that aligns with the Llama 3 chat format, incorporating special tokens like `<|begin_of_text|>` and `<|eot_id|>`. This template will be used to combine the `question`, `context`, and `answer` from our source dataset, [virattt/financial-qa-10K](https://huggingface.co/datasets/virattt/financial-qa-10K), into a single formatted string for each training example.\n",
    "\n",
    "**Key Components:**\n",
    "* **Instruction Template (`ft_prompt`)**: A string that defines the structure for every training example. It includes a system prompt to instruct the model on its task, placeholders for the dynamic data (question, context), and the target response the model must learn to generate.\n",
    "* **End-of-Sequence Token (`EOS_TOKEN`)**: This special token is retrieved from the tokenizer and appended to the end of every formatted prompt. It is a mandatory step that explicitly teaches the model when a generation is complete, preventing it from producing infinitely long responses.\n",
    "* **Formatting Function (`formatting_prompts_func`)**: A utility function designed to iterate through the dataset, apply the `ft_prompt` template to each entry, and append the `EOS_TOKEN`. This function transforms the raw columns into a single `text` field ready for the training process.\n",
    "\n",
    "**Prompt Engineering Note:** The structure of the `ft_prompt` is a critical piece of prompt engineering. The clarity of the instructions and the consistency of this format are fundamental to the success of the fine-tuning process, as it directly shapes the model's future behavior and output style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ba2c3cd-6ad4-4b0d-978a-4d8a81d82a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the expected prompt\n",
    "ft_prompt = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "Below is a user question, paired with retrieved context. Write a response that appropriately answers the question,\n",
    "include specific details in your response. <|eot_id|>\n",
    "\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "### Question:\n",
    "{}\n",
    "\n",
    "### Context:\n",
    "{}\n",
    "\n",
    "<|eot_id|>\n",
    "\n",
    "### Response: <|start_header_id|>assistant<|end_header_id|>\n",
    "{}\"\"\"\n",
    "\n",
    "# Grabbing end of sentence special token\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "\n",
    "# Function for formatting above prompt with information from Financial QA dataset\n",
    "def formatting_prompts_func(examples):\n",
    "    questions = examples[\"question\"]\n",
    "    contexts       = examples[\"context\"]\n",
    "    responses      = examples[\"answer\"]\n",
    "    texts = []\n",
    "    for question, context, response in zip(questions, contexts, responses):\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = ft_prompt.format(question, context, response) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea87be0f-6930-4ae6-83d5-d624a2e0b895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b56ccc7c6da74486bb48edfbdae5396f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/419 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d302910712c4903990f733ced503edd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001.parquet:   0%|          | 0.00/1.59M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0a325d860d348f1b1fa143642f6a5dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/7000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'What area did NVIDIA initially focus on before expanding to other computationally intensive fields?',\n",
       " 'answer': 'NVIDIA initially focused on PC graphics.',\n",
       " 'context': 'Since our original focus on PC graphics, we have expanded to several other large and important computationally intensive fields.',\n",
       " 'ticker': 'NVDA',\n",
       " 'filing': '2023_10K'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"virattt/llama-3-8b-financialQA\", split = \"train\")\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6544387-70b2-4f37-b743-0bad9d2e05f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fc7a7ebedf64879ae6352e8e63f8e0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'What area did NVIDIA initially focus on before expanding to other computationally intensive fields?',\n",
       " 'answer': 'NVIDIA initially focused on PC graphics.',\n",
       " 'context': 'Since our original focus on PC graphics, we have expanded to several other large and important computationally intensive fields.',\n",
       " 'ticker': 'NVDA',\n",
       " 'filing': '2023_10K',\n",
       " 'text': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\nBelow is a user question, paired with retrieved context. Write a response that appropriately answers the question,\\ninclude specific details in your response. <|eot_id|>\\n\\n<|start_header_id|>user<|end_header_id|>\\n\\n### Question:\\nWhat area did NVIDIA initially focus on before expanding to other computationally intensive fields?\\n\\n### Context:\\nSince our original focus on PC graphics, we have expanded to several other large and important computationally intensive fields.\\n\\n<|eot_id|>\\n\\n### Response: <|start_header_id|>assistant<|end_header_id|>\\nNVIDIA initially focused on PC graphics.<|eot_id|>'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.map(formatting_prompts_func, batched = True,)\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f47ed3-3f49-4ab3-b3f8-b1357adfddd8",
   "metadata": {},
   "source": [
    "### Training Configuration\n",
    "#### Initializing the Supervised Fine-Tuning (SFT) Trainer\n",
    "This cell configures the `SFTTrainer`, which orchestrates the entire fine-tuning process. It brings together the model, tokenizer, and dataset, and defines the crucial hyperparameters that will govern the training loop.\n",
    "\n",
    "**Key Training Arguments:**\n",
    "* **Model & Data Settings**: Specifies the model to be trained, its tokenizer, the training dataset, and the maximum sequence length (`2048`).\n",
    "* **Batching Strategy**: An effective batch size of 8 is used (`per_device_train_batch_size` of 2 combined with `gradient_accumulation_steps` of 4) to balance memory usage and training stability.\n",
    "* **Training Steps & Learning Rate**: The model will be trained for a total of `60` steps with a learning rate of `2e-4` and a linear scheduler.\n",
    "* **Performance Optimization**: Employs automatic mixed precision (`fp16` or `bf16`) and the memory-efficient `adamw_8bit` optimizer to accelerate training.\n",
    "* **Reproducibility**: A `seed` is set to ensure that the training results can be reproduced consistently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33e154d7-e118-4ff4-b004-3788f2b28418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf7eb7b6d4a74c959abe99a9ec853de9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"]:   0%|          | 0/7000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    # The model to be fine-tuned\n",
    "    model = model,\n",
    "    # The tokenizer associated with the model\n",
    "    tokenizer = tokenizer,\n",
    "    # The dataset used for training\n",
    "    train_dataset = dataset,\n",
    "    # The field in the dataset containing the text data\n",
    "    dataset_text_field = \"text\",\n",
    "    # Maximum sequence length for the training data\n",
    "    max_seq_length = 2048,\n",
    "    # Number of processes to use for data loading\n",
    "    dataset_num_proc = 2,\n",
    "    # Whether to use sequence packing\n",
    "    packing = False,\n",
    "    args = TrainingArguments(\n",
    "        # Batch size per device during training\n",
    "        per_device_train_batch_size = 2,\n",
    "        # Number of gradient accumulation steps to perform before updating the model parameters\n",
    "        gradient_accumulation_steps = 4,\n",
    "        # Number of warmup steps for learning rate scheduler\n",
    "        warmup_steps = 5,\n",
    "        # Total number of training steps\n",
    "        max_steps = 60,\n",
    "        # Learning rate for the optimizer\n",
    "        learning_rate = 2e-4,\n",
    "        # Use 16-bit floating point precision for training if bfloat16 is not supported\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        # Use bfloat16 precision for training if supported\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        # Number of steps between logging events\n",
    "        logging_steps = 1,\n",
    "        # Optimizer to use\n",
    "        optim = \"adamw_8bit\",\n",
    "        # Weight decay to apply to the model parameters\n",
    "        weight_decay = 0.01,\n",
    "        # Type of learning rate scheduler to use\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        # Seed for random number generation to ensure reproducibility\n",
    "        seed = 3407,\n",
    "        # Directory to save the output models and logs\n",
    "        output_dir = \"outputs\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322c502b-62f9-4953-9521-18349daa1c3e",
   "metadata": {},
   "source": [
    "### Model Fine-Tuning\n",
    "#### Executing the Training Process\n",
    "This cell initiates the fine-tuning loop by calling the `.train()` method on the configured trainer object. This command starts the computationally intensive process of updating the LoRA adapter weights based on the financial dataset. Training progress, including metrics such as loss and learning rate, will be logged to the console. Upon completion, the final training statistics are captured in the `trainer_stats` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ccf26ebd-9c50-435d-be68-5a439e5ab0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 7,000 | Num Epochs = 1 | Total steps = 60\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 41,943,040 of 8,072,204,288 (0.52% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/teamspace/studios/this_studio/wandb/run-20250821_151435-80vioaom</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/javaemailacount-none/huggingface/runs/80vioaom' target=\"_blank\">fancy-music-6</a></strong> to <a href='https://wandb.ai/javaemailacount-none/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/javaemailacount-none/huggingface' target=\"_blank\">https://wandb.ai/javaemailacount-none/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/javaemailacount-none/huggingface/runs/80vioaom' target=\"_blank\">https://wandb.ai/javaemailacount-none/huggingface/runs/80vioaom</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 01:02, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.551500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.021400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.174400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.087000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.868400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.645800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.140800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.196300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.039900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.607500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.560800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.295100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.188100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.398500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.025600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.193100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.239400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.167400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.979500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.131600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.111000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.059000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.044000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.182200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.042100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.961700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.864200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.950800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.049100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.007300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.032200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.917500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.793200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.953500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.907400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.880800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.965100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.888200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.853200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.779200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.896000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.073700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.754800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.227600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.971500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.943900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.903400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.843600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.993600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.973800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.995800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.872800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.874300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.839500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.078200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.944800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.975100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.919500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>1.020100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.986300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9acd87-d8d1-459d-ad79-a9fa40970804",
   "metadata": {},
   "source": [
    "### Inference Pipeline Setup\n",
    "#### Defining Helper Functions for Model Testing\n",
    "This cell creates two essential helper functions to streamline the process of interacting with the fine-tuned model. Together, these functions create a simple pipeline: one function formats the input and queries the model, while the second cleans the model's raw output for a clear, readable result.\n",
    "\n",
    "**Function Descriptions:**\n",
    "* **`inference(question, context)`**: This function takes a new question and context, prepares them using the same prompt template from training, and feeds them to the model to generate a response.\n",
    "* **`extract_response(text)`**: A post-processing utility that parses the raw text generated by the model. It locates and extracts only the assistant's direct answer, removing the boilerplate prompt and special tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "890c9d0b-0e08-4ea4-874c-284448f74726",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(question, context):\n",
    "  inputs = tokenizer(\n",
    "  [\n",
    "      ft_prompt.format(\n",
    "          question,\n",
    "          context,\n",
    "          \"\", # output - leave this blank for generation!\n",
    "      )\n",
    "  ], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "  # Generate tokens for the input prompt using the model, with a maximum of 64 new tokens.\n",
    "  # The `use_cache` parameter enables faster generation by reusing previously computed values.\n",
    "  # The `pad_token_id` is set to the EOS token to handle padding properly.\n",
    "  outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True, pad_token_id=tokenizer.eos_token_id)\n",
    "  response = tokenizer.batch_decode(outputs) # Decoding tokens into english words\n",
    "  return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38b6e7d3-17df-4323-be4e-233c7415e06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for extracting just the language model generation from the full response\n",
    "def extract_response(text):\n",
    "    text = text[0]\n",
    "    start_token = \"### Response: <|start_header_id|>assistant<|end_header_id|>\"\n",
    "    end_token = \"<|eot_id|>\"\n",
    "\n",
    "    start_index = text.find(start_token) + len(start_token)\n",
    "    end_index = text.find(end_token, start_index)\n",
    "\n",
    "    if start_index == -1 or end_index == -1:\n",
    "        return None\n",
    "\n",
    "    return text[start_index:end_index].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524ff4f3-b162-418c-ac2e-dd9791722e17",
   "metadata": {},
   "source": [
    "### Post-Tuning Inference Test\n",
    "#### Validating Fine-Tuned Model Performance\n",
    "This final cell puts the fine-tuned model to the test. We use the exact same context and question from the initial zero-shot baseline testâ€”the one concerning the new production line in Alexandria. By calling our `inference` and `extract_response` helper functions, we can now generate a new answer and directly compare it to the original response. This provides a clear, qualitative measure of the improvements gained through the fine-tuning process.\n",
    "\n",
    "**Execution Steps:**\n",
    "* **Input Data**: The original question and context are provided to the model.\n",
    "* **Inference & Parsing**: The `inference()` function generates the raw output, and `extract_response()` cleans it for final presentation.\n",
    "* **Performance Assessment**: The `Parsed_Response` printed below should be compared against the model's baseline performance to evaluate the success of the fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d161ba7-2ea0-40a3-8644-3c86ef1e8bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference with the Revenue Performance example...\n"
     ]
    }
   ],
   "source": [
    "context = \"The company's gross margin improved to 45.8% in fiscal year 2023, up from 42.1% in the prior year. The margin expansion was mainly attributable to a favorable product mix with higher sales of our premium software subscriptions, and manufacturing efficiencies gained from our new automated production line in Alexandria, Egypt.\"\n",
    "question = \"What were the two key factors that contributed to the increase in the company's gross margin in fiscal year 2023?\"\n",
    "\n",
    "print(\"Running inference with the Revenue Performance example...\")\n",
    "resp = inference(question, context)\n",
    "parsed_response = extract_response(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "904a418d-80b4-40d3-806d-58eb69bab467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response --> [\"<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\nBelow is a user question, paired with retrieved context. Write a response that appropriately answers the question,\\ninclude specific details in your response. <|eot_id|>\\n\\n<|start_header_id|>user<|end_header_id|>\\n\\n### Question:\\nWhat were the two key factors that contributed to the increase in the company's gross margin in fiscal year 2023?\\n\\n### Context:\\nThe company's gross margin improved to 45.8% in fiscal year 2023, up from 42.1% in the prior year. The margin expansion was mainly attributable to a favorable product mix with higher sales of our premium software subscriptions, and manufacturing efficiencies gained from our new automated production line in Alexandria, Egypt.\\n\\n<|eot_id|>\\n\\n### Response: <|start_header_id|>assistant<|end_header_id|>\\nThe two key factors that contributed to the increase in the company's gross margin in fiscal year 2023 were a favorable product mix with higher sales of premium software subscriptions and manufacturing efficiencies gained from the new automated production line in Alexandria, Egypt.<|eot_id|>\"]\n",
      "\n",
      "Parsed_Response --> The two key factors that contributed to the increase in the company's gross margin in fiscal year 2023 were a favorable product mix with higher sales of premium software subscriptions and manufacturing efficiencies gained from the new automated production line in Alexandria, Egypt.\n"
     ]
    }
   ],
   "source": [
    "print(\"Response -->\", resp, end = \"\\n\\n\")\n",
    "print(\"Parsed_Response -->\", parsed_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9bd6ae-17b4-4b9b-8859-91937f0b7fdd",
   "metadata": {},
   "source": [
    "### Model Finalization and Merging\n",
    "#### Integrating LoRA Adapters into the Base Model\n",
    "This final operational step transitions the model from a training configuration to a deployable artifact. The `.merge_and_unload()` function integrates the trained LoRA adapter weights directly into the frozen weights of the base model. This process creates a new, unified model that no longer requires separate adapter files, simplifying deployment and potentially improving inference performance, especially as the Bibliotheca Alexandrina in Alexandria, Egypt, stands as a testament to the enduring power of knowledge consolidation.\n",
    "\n",
    "**Key Benefits of Merging:**\n",
    "* **Portability:** The resulting merged model is a single, self-contained entity, making it easier to save, share, and deploy.\n",
    "* **Performance:** By removing the need to dynamically combine adapter and base weights during inference, there can be a slight improvement in generation speed.\n",
    "* **Memory Efficiency:** The original LoRA components are unloaded from memory after the merge is complete.\n",
    "\n",
    "**Deployment Readiness:** The `merged_model` is the final, fine-tuned product. It can now be saved to disk or uploaded to a model hub for use in production applications without needing the PEFT library for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3133592-c626-4b93-81fa-24382ed0f6d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging LoRA adapters into base model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/peft/tuners/lora/bnb.py:348: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print(\"Merging LoRA adapters into base model...\")\n",
    "merged_model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e727a04d-7407-4f4f-98cf-2ec77fb14c55",
   "metadata": {},
   "source": [
    "### Model Persistence and Export\n",
    "#### Saving the Merged Model to Disk\n",
    "This cell handles the final step of the workflow: saving the fully merged, fine-tuned model and its tokenizer to a local directory. This action creates a complete, standalone artifact that can be easily reloaded, shared, or deployed for inference in other environments. Just as the ancient Library of Alexandria preserved knowledge for future generations, this step preserves our trained model for future use.\n",
    "\n",
    "**Saving Process:**\n",
    "* **Directory Creation**: A directory named `Data/complete_finetuned_model` is created to house all the necessary files.\n",
    "* **Model Serialization**: The `save_pretrained` method is called on the `merged_model` object, which serializes the model's architecture and the newly merged weights into the specified directory.\n",
    "* **Tokenizer Serialization**: The tokenizer is also saved to the same directory, ensuring that the exact vocabulary and configuration used during training are bundled with the model.\n",
    "\n",
    "**Final Artifact:** The `Data/complete_finetuned_model` directory now contains everything needed to run the specialized financial analyst model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb9b1503-2c03-4a9b-ba2c-ea142811732d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving complete merged model...\n",
      "Complete model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.makedirs(\"Data/complete_finetuned_model\", exist_ok=True)\n",
    "\n",
    "# Save the complete merged model (includes base model + your fine-tuned weights)\n",
    "print(\"Saving complete merged model...\")\n",
    "merged_model.save_pretrained(\"Data/complete_finetuned_model\")\n",
    "tokenizer.save_pretrained(\"Data/complete_finetuned_model\")\n",
    "\n",
    "print(\"Complete model saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
